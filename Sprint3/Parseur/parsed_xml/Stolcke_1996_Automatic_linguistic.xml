<article>
  <preamble>Stolcke_1996_Automatic_linguistic.txt</preamble>
  <titre>AUTOMATIC LINGUISTIC SEGMENTATION OF CONVERSATIONAL SPEECH</titre>
  <auteur>                                           Andreas Stolcke                   Elizabeth Shriberg</auteur>
  <abstract>As speech recognition moves toward more unconstrained domains such as conversational speech, we encounter a need to be able to segment (or resegment) waveforms and recognizer output into lin- guistically meaningful units, such a sentences. Toward this end, we present a simple automatic segmenter of transcripts based on N-gram language modeling. We also study the relevance of sev- eral word-level features for segmentation performance. Using only word-level information, we achieve 85% recall and 70% precision on linguistic boundary detection.</abstract>
  <biblio>beled as either a segment boundary or a within-segment transition.
Two natural choices for alternative approaches are decision trees
and a transformation-based, error-driven classifier of the type de-
veloped by Eric Brill for other tagging problems [2]. Both of these
methods would make it easier to combine diverse input features that
are not readily integrated into a single probabilistic language model,
e.g., if we wanted to use both POS and word identity for each word.3
Our approach, on the other hand, has the advantage of simplicity
and efficiency. Furthermore, the language model used for segmen-
tation can also be used for speech decoding or rescoring.
We already mentioned that if POS information is to be used for
segmentation, an automatic tagging step is required. This presents
somewhat of a chicken-and-egg problem, in that taggers typically
rely on segmentations. An appealing solution to this problem in the
statistical tagging framework [3] would be to model both segmen-
tation and tag assignment as a single hidden Markov process.
6.3.     Other Features for Segmentation
All of our experiments were based on lexical information only. To
further improve segmentation performance, and to make it less de-
pendent on accurate speech recognition, we plan to combine the LM
approach with a model for various acoustic and prosodic correlates
of segmentation. These include:
 Unfilled pause durations
 Fundamental frequency patterns
 Phone durations
 Glottalization
Our current segmentation model deals with each conversation side
in isolation. An alternative approach is to model the two sides
jointly, thereby allowing us to capitalize on correlations between the
segment structure of one speaker and what is said by the other. It is
likely, for example, that backchannel responses would be modeled
better this way.
We have argued for the need for automatic speech segmentation al-
gorithms that can identify linguistically motivated, sentence-level
units of speech. We have shown that transcribed speech can be
segmented linguistically with good accuracy by using an N-gram
language model for the locations of the hidden segment boundaries.
We studied several word-level features for possible incorporation
in the model, and found that best performance so far was achieved
with a combination of function â€˜cueâ€™ words, POS labels, and turn
markers.
This research was supported by DARPA and NSF, under NSF Grant
IRI-9314967. The views herein are those of the authors and should
not be interpreted as representing the policies of DARPA or the
NSF.
3 Such an integration can be achieved in a language model using the max-
imum entropy paradigm [1], but this would make the estimation process
considerably more expensive.</biblio>
</article>
