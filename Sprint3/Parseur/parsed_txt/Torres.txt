Titre : Summary Evaluation with and without References

Abstract : the evaluation of text summarization systems without human models which is used to produce system rankings. The research is carried out using a new content-based evaluation framework called F RESA to compute a variety of divergences among probability distributions. We apply our comparison framework to various well-established content-based evaluation measures in text summarization such as C OVERAGE, R ESPONSIVENESS, P YRAMIDS and ROUGE studying their associations in various text summarization tasks including generic multi-document summarization in English and French, focus-based multi-document summarization in English and generic single-document summarization in French and Spanish. Index Terms—Text summarization evaluation, content-based evaluation measures, divergences.
Preamble : Torres.txt
Aueturs :    Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales
Biblio : Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales
Abstract—We study a new content-based method for
the evaluation of text summarization systems without
human models which is used to produce system rankings.
The research is carried out using a new content-based
evaluation framework called F RESA to compute a variety of
divergences among probability distributions. We apply our
comparison framework to various well-established content-based
evaluation measures in text summarization such as C OVERAGE,
R ESPONSIVENESS, P YRAMIDS and ROUGE studying their
associations in various text summarization tasks including
generic multi-document summarization in English and French,
focus-based multi-document summarization in English and
generic single-document summarization in French and Spanish.
Index Terms—Text summarization evaluation, content-based
evaluation measures, divergences.
T     EXT summarization evaluation has always been a
complex and controversial issue in computational
linguistics. In the last decade, significant advances have been
made in this field as well as various evaluation measures have
been designed. Two evaluation campaigns have been led by
the U.S. agence DARPA. The first one, SUMMAC, ran from
1996 to 1998 under the auspices of the Tipster program [1],
and the second one, entitled DUC (Document Understanding
Conference) [2], was the main evaluation forum from 2000
until 2007. Nowadays, the Text Analysis Conference (TAC)
[3] provides a forum for assessment of different information
access technologies including text summarization.
Evaluation in text summarization can be extrinsic or
intrinsic [4]. In an extrinsic evaluation, the summaries are
assessed in the context of an specific task carried out by a
human or a machine. In an intrinsic evaluation, the summaries
are evaluated in reference to some ideal model. SUMMAC
was mainly extrinsic while DUC and TAC followed an
intrinsic evaluation paradigm. In an intrinsic evaluation, an
Manuscript received June 8, 2010. Manuscript accepted for publication July
25, 2010.
Juan-Manuel Torres-Moreno is with LIA/Université d’Avignon,
France
(juan-manuel.torres@univ-avignon.fr).
Eric     SanJuan      is  with    LIA/Université    d’Avignon,    France
(eric.sanjuan@univ-avignon.fr).
Horacio Saggion is with DTIC/Universitat Pompeu Fabra, Spain
(horacio.saggion@upf.edu).
Iria da Cunha is with IULA/Universitat Pompeu Fabra, Spain;
LIA/Université d’Avignon, France and Instituto de Ingenierı́a/UNAM, Mexico
(iria.dacunha@upf.edu).
Patricia     Velázquez-Morales    is    with     VM
(patricia velazquez@yahoo.com).
Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales
of models and the identification, matching, and weighting of
SCUs in both: models and peers.
[12] evaluated the effectiveness of the Jensen-Shannon
(J S) [13] theoretic measure in predicting systems ranks
in two summarization tasks: query-focused and update
summarization. They have shown that ranks produced
by P YRAMIDS and those produced by J S measure
correlate. However, they did not investigate the effect
of the measure in summarization tasks such as generic
multi-document summarization (DUC 2004 Task 2),
biographical summarization (DUC 2004 Task 5), opinion
summarization (TAC 2008 OS), and summarization in
languages other than English.
In this paper we present a series of experiments aimed at
a better understanding of the value of the J S divergence
for ranking summarization systems. We have carried out
experimentation with the proposed measure and we have
verified that in certain tasks (such as those studied by
[12]) there is a strong correlation among P YRAMIDS,
R ESPONSIVENESS and the J S divergence, but as we will
show in this paper, there are datasets in which the correlation
is not so strong. We also present experiments in Spanish
and French showing positive correlation between the J S
and ROUGE which is the de facto evaluation measure used
in evaluation of non-English summarization. To the best of
our knowledge this is the more extensive set of experiments
interpreting the value of evaluation without human models.
The rest of the paper is organized in the following way:
First in Section II we introduce related work in the area of
content-based evaluation identifying the departing point for
our inquiry; then in Section III we explain the methodology
adopted in our work and the tools and resources used for
experimentation. In Section IV we present the experiments
carried out together with the results. Section V discusses the
results and Section VI concludes the paper and identifies future
work.
One of the first works to use content-based measures in
text summarization evaluation is due to [5], who presented an
evaluation framework to compare rankings of summarization
systems produced by recall and cosine-based measures. They
showed that there was weak correlation among rankings
produced by recall, but that content-based measures produce
rankings which were strongly correlated. This put forward
the idea of using directly the full document for comparison
purposes in text summarization evaluation. [6] presented a
set of evaluation measures based on the notion of vocabulary
overlap including n-gram overlap, cosine similarity, and
longest common subsequence, and they applied them to
multi-document summarization in English and Chinese.
However, they did not evaluate the performance of the
measures in different summarization tasks. [7] also compared
various evaluation measures based on vocabulary overlap.
Although these measures were able to separate random from
Polibits (42) 2010
while other measures compare peers with all or some of the
input material:
where Ii0 is some subset of input Ii . The values produced
by the measures for each summary SUMi,k are averaged
for each system k = 0, . . . , s − 1 and these averages are
used to produce a ranking. Rankings are then compared
using Spearman Rank correlation [17] which is used to
measure the degree of association between two variables
whose values are used to rank objects. We have chosen
to use this correlation to compare directly results to those
presented in [12]. Computation of correlations is done using
the Statistics-RankCorrelation-0.12 package1 , which computes
the rank correlation between two vectors. We also verified
the good conformity of the results with the correlation test
of Kendall τ calculated with the statistical software R. The
two nonparametric tests of Spearman and Kendall do not
really stand out as the treatment of ex-æquo. The good
correspondence between the two tests shows that they do not
introduce bias in our analysis. Subsequently will mention only
the ρ of Sperman more widely used in this field.
A. Tools
We carry out experimentation using a new summarization
evaluation framework: F RESA –FRamework for Evaluating
Summaries Automatically–, which includes document-based
summary evaluation measures based on probabilities
distribution2 . As in the ROUGE package, F RESA supports
different n-grams and skip n-grams probability distributions.
The F RESA environment can be used in the evaluation of
summaries in English, French, Spanish and Catalan, and it
integrates filtering and lemmatization in the treatment of
summaries and documents. It is developed in Perl and will
be made publicly available. We also use the ROUGE package
[10] to compute various ROUGE statistics in new datasets.
B. Summarization Tasks and Data Sets
We have conducted our experimentation with the following
summarization tasks and data sets:
1) Generic multi-document-summarization in English
(production of a short summary of a cluster of related
documents) using data from DUC’043 , task 2: 50
clusters, 10 documents each – 294,636 words.
2) Focused-based summarization in English (production of
a short focused multi-document summary focused on the
question “who is X?”, where X is a person’s name) using
data from the DUC’04 task 5: 50 clusters, 10 documents
each plus a target person name – 284,440 words.
1 http://search.cpan.org/∼gene/Statistics-RankCorrelation-0.12/
2 F RESA
Ressources.html
3 http://www-nlpir.nist.gov/projects/duc/guidelines/2004.html
Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales
– J S summarizer, a summarization system that scores
and ranks sentences according to their Jensen-Shannon
divergence to the source document;
– a lead-based summarization system that selects the lead
sentences of the document;
– a random-based summarization system that selects
sentences at random;
– Open Text Summarizer [23], a multi-lingual summarizer
based on the frequency and
– commercial systems: Word, SSSummarizer8 , Pertinence9
and Copernic10 .
C. Evaluation Measures
The following measures derived from human assessment of
the content of the summaries are used in our experiments:
– C OVERAGE is understood as the degree to which one
peer summary conveys the same information as a model
summary [2]. C OVERAGE was used in DUC evaluations.
This measure is used as indicated in equation 3 using
human references or models.
– R ESPONSIVENESS ranks summaries in a 5-point scale
indicating how well the summary satisfied a given
information need [2]. It is used in focused-based
summarization tasks. This measure is used as indicated
in equation 4 since a human judges the summary
with respect to a given input “user need” (e.g., a
question). R ESPONSIVENESS was used in DUC and TAC
evaluations.
– P YRAMIDS [11] is a content assessment measure which
compares content units in a peer summary to weighted
content units in a set of model summaries. This
measure is used as indicated in equation 3 using human
references or models. P YRAMIDS is the adopted metric
for content-based evaluation in the TAC evaluations.
For DUC and TAC datasets the values of these measures are
available and we used them directly. We used the following
automatic evaluation measures in our experiments:
– ROUGE [14], which is a recall metric that takes into
account n-grams as units of content for comparing peer
and model summaries. The ROUGE formula specified in
[10] is as follows:
8 http://www.kryltech.com/summarizer.htm
9 http://www.pertinence.net
10 http://www.copernic.com/en/products/summarizer
Polibits (42) 2010
S PEARMAN CORRELATION OF CONTENT- BASED MEASURES IN TAC’08
Mesure     P YRAMIDS      p-value   R ESPONSIVENESS     p-value
ROUGE-2
JS
multi-document summarization in Spanish and French. In spite
of the fact that the experiments for French and Spanish corpora
use less data points (i.e., less summarizers per task) than
for English, results are still quite significant. For DUC’04,
we computed the J S measure for each peer summary in
tasks 2 and 5 and we used J S, ROUGE, C OVERAGE and
R ESPONSIVENESS scores to produce systems’ rankings. The
various Spearman’s rank correlation values for DUC’04 are
presented in Tables II (for task 2) and III (for task 5).
For task 2, we have verified a strong correlation between
J S and C OVERAGE. For task 5, the correlation between
J S and C OVERAGE is weak, and that between J S and
R ESPONSIVENESS is weak and negative.
Although the Opinion Summarization (OS) task is a new
type of summarization task and its evaluation is a complicated
issue, we have decided to compare J S rankings with those
obtained using P YRAMIDS and R ESPONSIVENESS in TAC’08.
Spearman’s correlation values are listed in Table IV. As it can
be seen, there is weak and negative correlation of J S with
both P YRAMIDS and R ESPONSIVENESS. Correlation between
P YRAMIDS and R ESPONSIVENESS rankings is high for this
task (0.71 Spearman’s correlation value).
For experimentation in mono-document summarization
in Spanish and French, we have run 11 multi-lingual
summarization systems; for experimentation in French, we
have run 12 systems. In both cases, we have produced
summaries at a compression rate close to the compression rate
of the authors’ provided abstracts. We have then computed J S
and ROUGE measures for each summary and we have averaged
the measure’s values for each system. These averages were
used to produce rankings per each measure. We computed
Spearman’s correlations for all pairs of rankings.
Results are presented in Tables V, VI and VII. All results
show medium to strong correlation between the J S measures
and ROUGE measures. However the J S measure based on
uni-grams has lower correlation than J Ss which use n-grams
of higher order. Note that table VII presents results for
generic multi-document summarization in French, in this
case correlation scores are lower than correlation scores for
single-document summarization in French, a result which may
be expected given the diversity of input in multi-document
summarization.
The departing point for our inquiry into text summarization
evaluation has been recent work on the use of content-based
Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales
found weak correlation among different rankings in
complex summarization tasks such as the summarization
of biographical information and the summarization of
opinions.
– We have also carried out large-scale experiments in
Spanish and French which show positive medium to
strong correlation among system’s ranks produced by
ROUGE and divergence measures that do not use the
model summaries.
– We have also presented a new framework, F RESA, for
the computation of measures based on J S divergence.
Following the ROUGE approach, F RESA package use
word uni-grams, 2-grams and skip n-grams computing
divergences. This framework will be available to the
community for research purposes.
Although we have made a number of contributions, this paper
leaves many open questions than need to be addressed. In
order to verify correlation between ROUGE and J S, in the
short term we intend to extend our investigation to other
languages such as Portuguese and Chinesse for which we
have access to data and summarization technology. We also
plan to apply F RESA to the rest of the DUC and TAC
summarization tasks, by using several smoothing techniques.
As a novel idea, we contemplate the possibility of adapting
the evaluation framework for the phrase compression task
[29], which, to our knowledge, does not have an efficient
evaluation measure. The main idea is to calculate J S from
an automatically-compressed sentence taking the complete
sentence by reference. In the long term, we plan to incorporate
Polibits (42) 2010
[1] I. Mani, G. Klein, D. House, L. Hirschman, T. Firmin, and
B. Sundheim, “Summac: a text summarization evaluation,” Natural
Language Engineering, vol. 8, no. 1, pp. 43–68, 2002.
[2] P. Over, H. Dang, and D. Harman, “DUC in context,” IPM, vol. 43,
no. 6, pp. 1506–1520, 2007.
[3] Proceedings of the Text Analysis Conference. Gaithesburg, Maryland,
USA: NIST, November 17-19 2008.
[4] K. Spärck Jones and J. Galliers, Evaluating Natural Language
Processing Systems, An Analysis and Review, ser. Lecture Notes in
Computer Science. Springer, 1996, vol. 1083.
[5] R. L. Donaway, K. W. Drummey, and L. A. Mather, “A comparison of
rankings produced by summarization evaluation measures,” in NAACL
Workshop on Automatic Summarization, 2000, pp. 69–78.
[6] H. Saggion, D. Radev, S. Teufel, and W. Lam, “Meta-evaluation
of Summaries in a Cross-lingual Environment using Content-based
Metrics,” in COLING 2002, Taipei, Taiwan, August 2002, pp. 849–855.
[7] D. R. Radev, S. Teufel, H. Saggion, W. Lam, J. Blitzer, H. Qi, A. Çelebi,
D. Liu, and E. Drábek, “Evaluation challenges in large-scale document
summarization,” in ACL’03, 2003, pp. 375–382.
[8] K. Papineni, S. Roukos, T. Ward, , and W. J. Zhu, “BLEU: a method
for automatic evaluation of machine translation,” in ACL’02, 2002, pp.
311–318.
[9] K. Pastra and H. Saggion, “Colouring summaries BLEU,” in Evaluation
Initiatives in Natural Language Processing. Budapest, Hungary: EACL,
14 April 2003.
[10] C.-Y. Lin, “ROUGE: A Package for Automatic Evaluation of
Summaries,” in Text Summarization Branches Out: ACL-04 Workshop,
M.-F. Moens and S. Szpakowicz, Eds., Barcelona, July 2004, pp. 74–81.
[11] A. Nenkova and R. J. Passonneau, “Evaluating Content Selection in
Summarization: The Pyramid Method,” in HLT-NAACL, 2004, pp.
145–152.
[12] A. Louis and A. Nenkova, “Automatically Evaluating Content Selection
in Summarization without Human Models,” in Empirical Methods in
Natural Language Processing, Singapore, August 2009, pp. 306–314.
[Online]. Available: http://www.aclweb.org/anthology/D/D09/D09-1032
[13] J. Lin, “Divergence Measures based on the Shannon Entropy,” IEEE
Transactions on Information Theory, vol. 37, no. 145-151, 1991.
[14] C.-Y. Lin and E. Hovy, “Automatic Evaluation of Summaries Using
N-gram Co-occurrence Statistics,” in HLT-NAACL. Morristown, NJ,
USA: Association for Computational Linguistics, 2003, pp. 71–78.
[15] C.-Y. Lin, G. Cao, J. Gao, and J.-Y. Nie, “An information-theoretic
approach to automatic evaluation of summaries,” in HLT-NAACL,
Morristown, USA, 2006, pp. 463–470.
[16] S. Kullback and R. Leibler, “On information and sufficiency,” Ann. of
Math. Stat., vol. 22, no. 1, pp. 79–86, 1951.
[17] S. Siegel and N. Castellan, Nonparametric Statistics for the Behavioral
Sciences. McGraw-Hill, 1998.
